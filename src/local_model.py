from datetime import datetime
import json
from typing import List, Dict, Any
from llama_cpp import Llama
import os
import psutil
import GPUtil
import threading
import time

def get_timestamp():
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def check_system_info():
    info = []
    info.append(f"CPU –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: {psutil.cpu_percent()}%")
    try:
        gpus = GPUtil.getGPUs()
        for gpu in gpus:
            info.append(f"GPU {gpu.name}:")
            info.append(f"- –ó–∞–≥—Ä—É–∑–∫–∞ GPU: {gpu.load*100}%")
            info.append(f"- –ü–∞–º—è—Ç—å GPU: {gpu.memoryUsed}MB / {gpu.memoryTotal}MB")
    except:
        info.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU")
    return "\n".join(info)

class LocalModel:
    def __init__(self):
        self.model = None
        
    def initialize_model(self, model_path: str):
        try:
            self.model = Llama(
                model_path=model_path,
                n_ctx=16384,
                n_gpu_layers=35,
                n_threads=os.cpu_count(),
                n_batch=4096,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
                verbose=False,  # –í–∫–ª—é—á–∏—Ç–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                echo=False
            )
        except Exception as e:
            print(f"[{get_timestamp()}] üí• Error initializing model: {str(e)}")
            raise

    def generate_response(self, prompt: str, model_path: str, temperature: float = 0.8) -> str:
        print(f"[{get_timestamp()}] üì° generate model: {model_path.split('/')[-1]}")
        if not self.model or self.model.model_path != model_path:
            self.initialize_model(model_path)
            
        try:
            output = self.model(
                prompt,
                temperature=temperature,
                max_tokens=16384  # –£–≤–µ–ª–∏—á—å—Ç–µ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
            )
            response = output["choices"][0]["text"].strip()
            return response
        except Exception as e:
            print(f"[{get_timestamp()}] üí• Error generating response: {str(e)}")
            raise

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    model_path = "C:/Users/home/.lmstudio/models/lmstudio-community/Qwen2.5-7B-Instruct-1M-GGUF/Qwen2.5-7B-Instruct-1M-Q4_K_M.gguf"
    prompt = "–ù–∞–∑–≤–∞–Ω–∏–µ: –ë–æ–ª—å—à–æ–π –°–∏–±–∏—Ä—Å–∫–∏–π –°—Ç–µ–Ω–¥–∞–ø | –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫..."  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à –ø—Ä–æ–º–ø—Ç
    
    local_model = LocalModel()
    local_model.initialize_model(model_path)
    
    def generate():
        response = local_model.generate_response(prompt, model_path)
        print(f"[{get_timestamp()}] –û—Ç–≤–µ—Ç: {response}")

    thread = threading.Thread(target=generate)
    thread.start()

    while thread.is_alive():
        print(f"[{get_timestamp()}] –°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:")
        print(check_system_info())
        time.sleep(1)

    thread.join()